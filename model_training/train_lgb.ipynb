{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff1b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from helper import log_multi_target_run, transform_targets, compute_metrics\n",
    "from tiny_mlflow import log_multi_target_run_local\n",
    "import gc\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8b33ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DROP = ['id']\n",
    "TARGETS = [\"n2o\", \"no3\", \"yield\", \"soc\"]\n",
    "\n",
    "gdf = cudf.read_parquet(\"data/final_processed_data_201125.parquet\")\n",
    "\n",
    "X_train = gdf.drop(columns=TARGETS+DROP)\n",
    "y_train = gdf[TARGETS]\n",
    "\n",
    "del gdf\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a52406-5a0e-43a8-bdab-016fa7c66c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['soil', 'climate', 'cropping_systems', 'crop_rotation', 'n_synth_type',\n",
       "       'n_org_type', 'n_org_replication', 'n_synth_replication', 'irrigation',\n",
       "       'manu_depth', 'n_org_amount', 'n_synthamount', 'id', 'fert_amount_1',\n",
       "       'fert_amount_2', 'fert_amount_3', 'manu_amount_1', 'manu_amount_2',\n",
       "       'manu_amount_3', 'prec_days', 'total_nitrogen',\n",
       "       'total_precipitation_year', 'total_average_temperature_year',\n",
       "       'total_precipitation_growing_season',\n",
       "       'total_average_temperature_growing_season', 'total_precipitation_autum',\n",
       "       'total_average_temperature_autum', 'total_precipitation_winter',\n",
       "       'total_average_temperature_winter', 'total_precipitation_spring',\n",
       "       'total_average_temperature_spring', 'bd', 'corg', 'norg', 'sand',\n",
       "       'silt', 'clay', 'ph', 'sks', 'wcmax', 'wcmin',\n",
       "       'total_precipitation_3_after_fert_1',\n",
       "       'total_precipitation_3_after_fert_2',\n",
       "       'total_precipitation_3_after_fert_3',\n",
       "       'total_precipitation_3_after_manu_1',\n",
       "       'total_precipitation_3_after_manu_2',\n",
       "       'total_precipitation_3_after_manu_3',\n",
       "       'total_precipitation_7_before_fert_1',\n",
       "       'total_precipitation_7_before_fert_2',\n",
       "       'total_precipitation_7_before_fert_3',\n",
       "       'total_precipitation_7_before_manu_1',\n",
       "       'total_precipitation_7_before_manu_2',\n",
       "       'total_precipitation_7_before_manu_3', 'synth_org_ratio',\n",
       "       'precipitation_clay_interaction', 'precip_n_interaction',\n",
       "       'fert_amount_1_sq', 'fert_amount_2_sq', 'fert_amount_3_sq',\n",
       "       'manu_amount_1_sq', 'manu_amount_2_sq', 'manu_amount_3_sq',\n",
       "       'total_nitrogen_sq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c2afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_models(X_train_cudf, y_train_cudf, log_to_mlflow=False):\n",
    "    # LightGBM works nicely with pandas DataFrame\n",
    "    X_pd = X_train_cudf.to_pandas()\n",
    "    feature_names = list(X_train_cudf.columns)\n",
    "\n",
    "    # detect categorical features by column names\n",
    "    # If you encoded categoricals to int codes already, you may want to\n",
    "    # build this from your known cat_cols instead of dtypes.\n",
    "    cat_features = [\n",
    "        col for col in X_pd.columns\n",
    "        if str(X_pd[col].dtype) == \"category\" or X_pd[col].dtype == \"object\"\n",
    "    ]\n",
    "\n",
    "    # transform targets: log1p for n2o/no3/yield, Yeo Johnson for soc\n",
    "    y_orig_np, y_trans_np, transformers = transform_targets(y_train_cudf)\n",
    "\n",
    "    models = {}\n",
    "    metrics = {}\n",
    "    model_name = \"lightgbm_v1\"\n",
    "\n",
    "    for target in TARGETS:\n",
    "        print(f\"Training LightGBM for target '{target}'...\")\n",
    "\n",
    "        y_trans = y_trans_np[target]\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            metric=\"rmse\",\n",
    "            boosting_type=\"gbdt\",\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.01,\n",
    "            num_leaves=256,\n",
    "            max_depth=-1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            reg_alpha=0.0,\n",
    "            random_state=42,\n",
    "            device_type=\"gpu\",   # set to \"cpu\" if GPU is not available\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_pd,\n",
    "            y_trans,\n",
    "            categorical_feature=cat_features,\n",
    "        )\n",
    "\n",
    "        preds_trans = model.predict(X_pd)\n",
    "\n",
    "        metrics[target], _ = compute_metrics(\n",
    "            target,\n",
    "            preds_trans,\n",
    "            y_orig_np[target],\n",
    "            transformers[target],\n",
    "            model_name,\n",
    "        )\n",
    "\n",
    "        models[target] = model\n",
    "\n",
    "        print(f\"Done training LightGBM for target '{target}'\")\n",
    "\n",
    "    if log_to_mlflow:\n",
    "        log_multi_target_run(\n",
    "            model_family_name=\"lightgbm\",\n",
    "            models=models,\n",
    "            metrics=metrics,\n",
    "            feature_names=feature_names,\n",
    "            transformers=transformers,\n",
    "            experiment_name=\"multi_target_lightgbm\",\n",
    "            run_name=\"lightgbm_train_full\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Local JSON logger\n",
    "        log_multi_target_run_local(\n",
    "            model_family_name=\"lightgbm\",\n",
    "            models=models,\n",
    "            metrics=metrics,\n",
    "            feature_names=feature_names,\n",
    "            transformers=transformers,\n",
    "            experiment_name=\"multi_target_lightgbm\",\n",
    "            run_name=\"lightgbm_train_full\",\n",
    "            base_dir=\"experiments\",  # optional\n",
    "        )\n",
    "\n",
    "    return models, metrics, transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d871f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM for target 'n2o'...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11172\n",
      "[LightGBM] [Info] Number of data points in the train set: 45265130, number of used features: 62\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA H100 80GB HBM3, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 52 dense feature groups (2244.75 MB) transferred to GPU in 0.773909 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.877517\n",
      "Done training LightGBM for target 'n2o'\n",
      "Training LightGBM for target 'no3'...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11172\n",
      "[LightGBM] [Info] Number of data points in the train set: 45265130, number of used features: 62\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA H100 80GB HBM3, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 52 dense feature groups (2244.75 MB) transferred to GPU in 0.771565 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 2.252057\n",
      "Done training LightGBM for target 'no3'\n",
      "Training LightGBM for target 'yield'...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11172\n",
      "[LightGBM] [Info] Number of data points in the train set: 45265130, number of used features: 62\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA H100 80GB HBM3, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 52 dense feature groups (2244.75 MB) transferred to GPU in 0.762404 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 9.690798\n",
      "Done training LightGBM for target 'yield'\n",
      "Training LightGBM for target 'soc'...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11172\n",
      "[LightGBM] [Info] Number of data points in the train set: 45265130, number of used features: 62\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA H100 80GB HBM3, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 52 dense feature groups (2244.75 MB) transferred to GPU in 0.753072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Done training LightGBM for target 'soc'\n",
      "LightGBM metrics: {'n2o': {'rmse': 0.48751984941876625, 'mae': 0.3273579492573637, 'r2': 0.8136504472892375}, 'no3': {'rmse': 19.66150877326015, 'mae': 9.811464371208036, 'r2': 0.8284119571283766}, 'yield': {'rmse': 543.8640345375795, 'mae': 373.9877374543775, 'r2': 0.9508101290088917}, 'soc': {'rmse': 293.3045270649309, 'mae': 216.42762722652768, 'r2': 0.859407078244525}}\n"
     ]
    }
   ],
   "source": [
    "lgb_models, lgb_metrics, lgb_transformers = train_lgb_models(X_train, y_train, False)\n",
    "\n",
    "print(\"LightGBM metrics:\", lgb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24221b6-5038-49bf-939a-a97d42eb3c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mldndc-env)",
   "language": "python",
   "name": "mldndc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
